{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/names.txt\", \"r\") as file:\n",
    "    words = file.read().splitlines()\n",
    "\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(\"\".join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "print(stoi)\n",
    "print(itos)\n",
    "vocab_size = len(stoi)\n",
    "print(\"vocab_size: \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 3\n",
    "\n",
    "X, Y = [], []\n",
    "\n",
    "for w in words[:3]:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        print(\"\".join(itos[i] for i in context), \"--->\", itos[ix])\n",
    "        context = context[1:] + [ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataset\n",
    "block_size = 3\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, X.dtype, Y.shape, Y.dtype)\n",
    "    return X, Y\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(7):\n",
    "    print(\"Input: \", Xtr[i].tolist(),\" Target: \", Ytr[i].item())\n",
    "    print(\"Input: \", \"\".join((itos[w]) for w in Xtr[i].tolist()), \" Target: \", itos[Ytr[i].item()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, block_size, n_hidden, use_norm_layer=True):\n",
    "        super(MLP, self).__init__()\n",
    "        # Create the embedding table\n",
    "        self.embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        \n",
    "        # Define the sequence of layers\n",
    "        layers = [\n",
    "            nn.Linear(n_embd * block_size, n_hidden),\n",
    "            nn.LayerNorm(n_hidden) if use_norm_layer else nn.Identity(),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.LayerNorm(n_hidden) if use_norm_layer else nn.Identity(),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.LayerNorm(n_hidden) if use_norm_layer else nn.Identity(),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.LayerNorm(n_hidden) if use_norm_layer else nn.Identity(),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.LayerNorm(n_hidden) if use_norm_layer else nn.Identity(),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_hidden, vocab_size),\n",
    "            nn.LayerNorm(vocab_size) if use_norm_layer else nn.Identity()\n",
    "        ]\n",
    "        \n",
    "        # Create a sequential container\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward pass through the embedding table\n",
    "        x = self.embedding_table(x)  # shape: (batch_size, block_size, n_embd)\n",
    "        # Flatten the embeddings\n",
    "        x = x.view(x.size(0), -1)  # shape: (batch_size, block_size * n_embd)\n",
    "        # Forward pass through the MLP layers\n",
    "        x = self.layers(x)  # shape: (batch_size, vocab_size)\n",
    "        return x\n",
    "\n",
    "# Define parameters\n",
    "vocab_size = 27      # as per your requirement\n",
    "n_embd = 10          # you can choose this value as needed\n",
    "block_size = 4       # number of input characters\n",
    "n_hidden = 200       # size of the hidden layer\n",
    "use_norm_layer = True  # flag to use normalization layer\n",
    "lr_init = 0.01\n",
    "\n",
    "# Check for CUDA\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create an instance of the MLP class\n",
    "model = MLP(vocab_size, n_embd, block_size, n_hidden, use_norm_layer).to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr_init, weight_decay=0.01, betas=(0.9, 0.999), eps=1e-08)\n",
    "\n",
    "# Make datasets\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Parameters for the learning rate finder\n",
    "lr_min = 1e-7\n",
    "lr_max = 1e-1\n",
    "num_iters = 1000\n",
    "batch_size = 32\n",
    "log_lrs = []\n",
    "losses = []\n",
    "\n",
    "# Define a function to test different learning rates\n",
    "def find_lr(model, Xtr, Ytr, lr_min, lr_max, num_iters, batch_size):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr_min)\n",
    "    lr_lambda = lambda x: (lr_max / lr_min) ** (x / num_iters)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Minibatch\n",
    "        ix = torch.randint(0, Xtr.shape[0], (batch_size,), device=device)\n",
    "        Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(Xb)\n",
    "        loss = F.cross_entropy(logits, Yb)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Track learning rates and losses\n",
    "        log_lrs.append(np.log10(optimizer.param_groups[0]['lr']))\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Print progress\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Step {i + 1}/{num_iters}, Loss: {loss.item()}, LR: {optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "# Create an instance of the MLP class\n",
    "model = MLP(vocab_size, n_embd, block_size, n_hidden, use_norm_layer).to(device)\n",
    "\n",
    "# Move datasets to the appropriate device\n",
    "Xtr, Ytr = Xtr.to(device), Ytr.to(device)\n",
    "\n",
    "# Run the learning rate finder\n",
    "find_lr(model, Xtr, Ytr, lr_min, lr_max, num_iters, batch_size)\n",
    "\n",
    "# Plot the learning rate finder results with Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=log_lrs,\n",
    "    y=losses,\n",
    "    mode='lines+markers',\n",
    "    name='Loss'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Learning Rate Finder',\n",
    "    xaxis_title='Learning Rate (log scale)',\n",
    "    yaxis_title='Loss',\n",
    "    hovermode='x'\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 400000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "Xtr, Ytr = Xtr.to(device), Ytr.to(device)\n",
    "\n",
    "for i in range(max_steps):\n",
    "    # Minibatch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), device=device)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "    # Forward pass\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(Xb)\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update\n",
    "    if i == int(0.67 * max_steps):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr_init / 10\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Track stats\n",
    "    lossi.append(loss.item())\n",
    "    if i % 5000 == 0:\n",
    "        mean_loss = sum(lossi[-100:]) / 100\n",
    "        print(f\"{i:7d} / {max_steps:7d}: Mean Loss: {mean_loss:.4f}\")\n",
    "\n",
    "    # if i > 10000:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate moving average\n",
    "def moving_average(values, window):\n",
    "    moving_avgs = []\n",
    "    for i in range(len(values)):\n",
    "        if i < window:\n",
    "            moving_avgs.append(sum(values[:i+1]) / (i+1))\n",
    "        else:\n",
    "            moving_avgs.append(sum(values[i-window+1:i+1]) / window)\n",
    "    return moving_avgs\n",
    "\n",
    "# Calculate moving average with window size 100\n",
    "smoothed_loss = moving_average(lossi, 100)\n",
    "\n",
    "# Plotting the smoothed loss curve\n",
    "plt.plot(smoothed_loss)\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xdev, Ydev = Xdev.to(device), Ydev.to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "    x,y = {\n",
    "        \"train\": (Xtr, Ytr),\n",
    "        \"val\": (Xdev, Ydev),\n",
    "        \"test\": (Xte, Yte)\n",
    "    }[split]\n",
    "    x, y = x.to(device=device), y.to(device=device)\n",
    "    # forward pass\n",
    "    logits = model(x)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "\n",
    "\n",
    "    # # -------------\n",
    "    # emb = C[x]\n",
    "    # embcat = emb.view(emb.shape[0], -1)\n",
    "    # h = torch.tanh(embcat @ W1 + b1)  \n",
    "    # logits = h @ W2 + b2                                           \n",
    "    # loss = F.cross_entropy(logits, y)\n",
    "    print(f\"{split}: {loss.item():.4f}\")\n",
    "\n",
    "split_loss(\"train\")\n",
    "split_loss(\"val\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_embd == 2:\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.scatter(C[:,0].data, C[:,1].data, s=200)\n",
    "    for i in range(C.shape[0]):\n",
    "        plt.text(C[i,0].item(), C[i,1].item(), itos[i], ha=\"center\", va=\"center\", color=\"white\")\n",
    "    plt.grid(\"minor\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling function\n",
    "with torch.no_grad():\n",
    "    for _ in range(20):\n",
    "        model.eval()\n",
    "        out = []\n",
    "        context = [0] * block_size\n",
    "        while True:\n",
    "            x = model(torch.tensor([context]).to(device=device))\n",
    "            probs = F.softmax(x, dim=1)\n",
    "            ix = torch.multinomial(probs, num_samples=1).item()\n",
    "            context = context[1:] + [ix]\n",
    "            out.append(ix)\n",
    "            if ix == 0:\n",
    "                break\n",
    "        print(\"\".join(itos[i] for i in out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "make-gpt-h_QPYcFF-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
